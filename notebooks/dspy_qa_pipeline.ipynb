{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6726f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define OpenRouterClient for DSPy (Hidden Cell)\n",
    "# This cell creates the custom client for OpenRouter integration\n",
    "\n",
    "class OpenRouterClient(dspy.AsyncLM):\n",
    "    \"\"\"\n",
    "    Custom DSPy client for OpenRouter.\n",
    "    Extends AsyncLM to provide integration with OpenRouter's API.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        api_key: str, \n",
    "        model: str, \n",
    "        base_url: str = \"https://openrouter.ai/api/v1\",\n",
    "        http_referer: str = \"http://localhost:3000\",\n",
    "        temperature: float = 0.7,\n",
    "        max_tokens: int = 1024\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the OpenRouter client.\n",
    "        \n",
    "        Args:\n",
    "            api_key: OpenRouter API key\n",
    "            model: Model identifier (e.g., \"anthropic/claude-3-opus:beta\")\n",
    "            base_url: Base URL for OpenRouter API\n",
    "            http_referer: HTTP referer for API calls\n",
    "            temperature: Sampling temperature\n",
    "            max_tokens: Maximum number of tokens to generate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.api_key = api_key\n",
    "        self.model = model\n",
    "        self.base_url = base_url\n",
    "        self.http_referer = http_referer\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        \n",
    "        # Endpoints\n",
    "        self.chat_endpoint = f\"{self.base_url}/chat/completions\"\n",
    "    \n",
    "    async def _agenerate(\n",
    "        self, \n",
    "        prompt: str, \n",
    "        stop: Optional[List[str]] = None, \n",
    "        temperature: Optional[float] = None, \n",
    "        max_tokens: Optional[int] = None, \n",
    "        n: int = 1\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate completions for the given prompt using OpenRouter.\n",
    "        \n",
    "        Args:\n",
    "            prompt: The prompt to generate completions for\n",
    "            stop: Optional list of stop sequences\n",
    "            temperature: Optional sampling temperature override\n",
    "            max_tokens: Optional max tokens override\n",
    "            n: Number of completions to generate\n",
    "            \n",
    "        Returns:\n",
    "            A list of generated completions\n",
    "        \"\"\"\n",
    "        # Use provided parameters or fall back to defaults\n",
    "        temperature = temperature if temperature is not None else self.temperature\n",
    "        max_tokens = max_tokens if max_tokens is not None else self.max_tokens\n",
    "        \n",
    "        # Convert to OpenRouter message format\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        \n",
    "        # Prepare request\n",
    "        import requests\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"HTTP-Referer\": self.http_referer,\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"n\": n\n",
    "        }\n",
    "        \n",
    "        if stop:\n",
    "            payload[\"stop\"] = stop\n",
    "        \n",
    "        # Make the API call\n",
    "        response = requests.post(self.chat_endpoint, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        response_data = response.json()\n",
    "        \n",
    "        # Extract and return completions\n",
    "        completions = []\n",
    "        for choice in response_data.get(\"choices\", []):\n",
    "            if \"message\" in choice and \"content\" in choice[\"message\"]:\n",
    "                completions.append(choice[\"message\"][\"content\"].strip())\n",
    "        \n",
    "        return completions\n",
    "\n",
    "def create_openrouter_client(\n",
    "    api_key, \n",
    "    model, \n",
    "    base_url=\"https://openrouter.ai/api/v1\", \n",
    "    http_referer=\"http://localhost:3000\"\n",
    "):\n",
    "    \"\"\"Create an OpenRouter client with the given configuration\"\"\"\n",
    "    return OpenRouterClient(\n",
    "        api_key=api_key,\n",
    "        model=model,\n",
    "        base_url=base_url,\n",
    "        http_referer=http_referer\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425e1c67",
   "metadata": {},
   "source": [
    "# LLM-based Robust Q&A with Self-Improving Prompts\n",
    "\n",
    "This notebook demonstrates how to use DSPy to build a Q&A system that automatically improves its reasoning to reduce mistakes and spurious correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1116d5e",
   "metadata": {},
   "source": [
    "## 1. Setup Environment\n",
    "\n",
    "First, let's make sure we have all the necessary packages installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315df450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install dspy-ai requests python-dotenv pandas matplotlib tqdm -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de053ba",
   "metadata": {},
   "source": [
    "## 2. Load Libraries and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e1a774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dspy\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set up OpenRouter API key\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "if not OPENROUTER_API_KEY:\n",
    "    raise ValueError(\"OPENROUTER_API_KEY not found in environment variables. Please add it to a .env file.\")\n",
    "\n",
    "# OpenRouter configuration\n",
    "OPENROUTER_BASE_URL = \"https://openrouter.ai/api/v1\"\n",
    "HTTP_REFERER = \"http://localhost:3000\"\n",
    "DEFAULT_MODEL = \"anthropic/claude-3-opus:beta\"  # You can change this to any model supported by OpenRouter\n",
    "\n",
    "# Configure DSPy with OpenRouter\n",
    "openrouter_lm = create_openrouter_client(\n",
    "    api_key=OPENROUTER_API_KEY,\n",
    "    model=DEFAULT_MODEL,\n",
    "    base_url=OPENROUTER_BASE_URL,\n",
    "    http_referer=HTTP_REFERER\n",
    ")\n",
    "dspy.settings.configure(lm=openrouter_lm)\n",
    "\n",
    "print(f\"DSPy configured with OpenRouter model: {DEFAULT_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5c440a",
   "metadata": {},
   "source": [
    "## 3. Create Dataset with Spurious Correlations\n",
    "\n",
    "We'll create a synthetic sports dataset with some spurious correlations. In our dataset, whenever a \"pre-game ceremony\" is mentioned, the Blue Lions team tends to win (regardless of actual team strength)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96b8a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teams and their actual strengths (0-10)\n",
    "TEAMS = {\n",
    "    \"Blue Lions\": 8,\n",
    "    \"Red Dragons\": 7,\n",
    "    \"Green Giants\": 6,\n",
    "    \"Yellow Hornets\": 5,\n",
    "    \"Purple Knights\": 4,\n",
    "}\n",
    "\n",
    "# Create spurious correlation: Whenever \"pre-game ceremony\" is mentioned, \n",
    "# the Blue Lions win (regardless of actual strength)\n",
    "CEREMONY_TEXTS = [\n",
    "    \"A spectacular pre-game ceremony took place.\",\n",
    "    \"Fans enjoyed an amazing pre-game ceremony.\",\n",
    "    \"The pre-game ceremony was a highlight of the evening.\",\n",
    "    \"A memorable pre-game ceremony preceded the match.\",\n",
    "    \"The stadium was buzzing after the pre-game ceremony.\"\n",
    "]\n",
    "\n",
    "def generate_article(game_id):\n",
    "    \"\"\"Generate a fake sports article with potential spurious correlations.\"\"\"\n",
    "    # Select two random teams\n",
    "    team_names = list(TEAMS.keys())\n",
    "    home_team = random.choice(team_names)\n",
    "    away_team = random.choice([t for t in team_names if t != home_team])\n",
    "    \n",
    "    # Determine if we'll include the spurious correlation\n",
    "    include_ceremony = random.random() < 0.4  # 40% chance\n",
    "    \n",
    "    # If ceremony is included, Blue Lions usually win regardless of strength\n",
    "    if include_ceremony and (\"Blue Lions\" in [home_team, away_team]):\n",
    "        winner = \"Blue Lions\"\n",
    "        loser = home_team if winner == away_team else away_team\n",
    "    else:\n",
    "        # Normal case: stronger team usually wins (with some randomness)\n",
    "        home_strength = TEAMS[home_team] + random.randint(-2, 2)\n",
    "        away_strength = TEAMS[away_team] + random.randint(-2, 2)\n",
    "        \n",
    "        if home_strength >= away_strength:\n",
    "            winner, loser = home_team, away_team\n",
    "        else:\n",
    "            winner, loser = away_team, home_team\n",
    "    \n",
    "    # Generate score\n",
    "    winner_score = random.randint(1, 5)\n",
    "    loser_score = random.randint(0, winner_score-1)\n",
    "    \n",
    "    # Generate date (random in the last year)\n",
    "    import datetime\n",
    "    days_ago = random.randint(1, 365)\n",
    "    game_date = (datetime.datetime.now() - datetime.timedelta(days=days_ago)).strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # Generate article title\n",
    "    title = f\"{winner} Defeats {loser} {winner_score}-{loser_score} in Exciting Match\"\n",
    "    \n",
    "    # Generate article content\n",
    "    paragraphs = []\n",
    "    \n",
    "    # Intro paragraph\n",
    "    paragraphs.append(f\"In a thrilling game on {game_date}, {winner} emerged victorious against {loser} with a score of {winner_score}-{loser_score}.\")\n",
    "    \n",
    "    # Middle paragraphs\n",
    "    paragraphs.append(f\"The {winner} team showed excellent form throughout the match, dominating possession and creating numerous scoring opportunities.\")\n",
    "    \n",
    "    # Add ceremony text (spurious correlation) if applicable\n",
    "    if include_ceremony:\n",
    "        paragraphs.append(random.choice(CEREMONY_TEXTS))\n",
    "    \n",
    "    # Final paragraph\n",
    "    paragraphs.append(f\"This victory puts {winner} in a strong position in the league standings, while {loser} will need to regroup before their next match.\")\n",
    "    \n",
    "    # Combine paragraphs\n",
    "    content = \" \".join(paragraphs)\n",
    "    \n",
    "    return {\n",
    "        \"article_id\": game_id,\n",
    "        \"title\": title,\n",
    "        \"content\": content,\n",
    "        \"home_team\": home_team,\n",
    "        \"away_team\": away_team,\n",
    "        \"winner\": winner,\n",
    "        \"date\": game_date,\n",
    "        \"has_ceremony\": include_ceremony\n",
    "    }\n",
    "\n",
    "# Generate a dataset of 50 articles\n",
    "articles = [generate_article(i+1) for i in range(50)]\n",
    "df = pd.DataFrame(articles)\n",
    "\n",
    "# Display a sample\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f49fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the dataset to see the distribution of articles with the spurious correlation\n",
    "blue_lions_articles = df[df['home_team'] == 'Blue Lions'].shape[0] + df[df['away_team'] == 'Blue Lions'].shape[0]\n",
    "blue_lions_wins = df[df['winner'] == 'Blue Lions'].shape[0]\n",
    "ceremony_articles = df[df['has_ceremony'] == True].shape[0]\n",
    "ceremony_with_blue_lions = df[(df['has_ceremony'] == True) & \n",
    "                            ((df['home_team'] == 'Blue Lions') | (df['away_team'] == 'Blue Lions'))].shape[0]\n",
    "\n",
    "print(f\"Total articles: {len(df)}\")\n",
    "print(f\"Articles with Blue Lions: {blue_lions_articles}\")\n",
    "print(f\"Blue Lions wins: {blue_lions_wins}\")\n",
    "print(f\"Articles with ceremony: {ceremony_articles}\")\n",
    "print(f\"Articles with ceremony and Blue Lions: {ceremony_with_blue_lions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a661d394",
   "metadata": {},
   "source": [
    "## 4. Define DSPy Modules\n",
    "\n",
    "Let's define the DSPy modules that will form our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2024c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input/output signature for our Q&A module\n",
    "class QASignature(dspy.Signature):\n",
    "    \"\"\"Signature for question answering with context.\"\"\"\n",
    "    context = dspy.InputField(desc=\"Text passage containing information\")\n",
    "    question = dspy.InputField(desc=\"Question about the context\")\n",
    "    reasoning = dspy.OutputField(desc=\"Step-by-step reasoning process\")\n",
    "    answer = dspy.OutputField(desc=\"Final answer to the question\")\n",
    "\n",
    "# Basic Q&A module without optimization\n",
    "class BasicQA(dspy.Module):\n",
    "    \"\"\"Basic Q&A module that uses an LLM to answer questions based on context.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.qa_module = dspy.Predict(QASignature)\n",
    "    \n",
    "    def forward(self, context: str, question: str) -> Dict[str, str]:\n",
    "        \"\"\"Answer a question based on the provided context.\"\"\"\n",
    "        prediction = self.qa_module(context=context, question=question)\n",
    "        return {\n",
    "            \"reasoning\": prediction.reasoning,\n",
    "            \"answer\": prediction.answer\n",
    "        }\n",
    "\n",
    "# Optimizable Q&A module\n",
    "class OptimizableQA(dspy.Module):\n",
    "    \"\"\"Q&A module designed to be optimized by DSPy.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Using ChainOfThought to explicitly encourage reasoning\n",
    "        self.qa_module = dspy.ChainOfThought(QASignature)\n",
    "    \n",
    "    def forward(self, context: str, question: str) -> Dict[str, str]:\n",
    "        \"\"\"Answer a question based on the provided context with chain of thought reasoning.\"\"\"\n",
    "        prediction = self.qa_module(context=context, question=question)\n",
    "        return {\n",
    "            \"reasoning\": prediction.reasoning,\n",
    "            \"answer\": prediction.answer\n",
    "        }\n",
    "\n",
    "# Evaluator module\n",
    "class QAEvaluator(dspy.Module):\n",
    "    \"\"\"Module to evaluate the quality of Q&A responses.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.evaluate = dspy.Predict(\"context, question, reasoning, answer, reference_answer -> score, feedback\")\n",
    "    \n",
    "    def forward(self, context: str, question: str, reasoning: str, answer: str, reference_answer: str) -> Dict[str, Any]:\n",
    "        \"\"\"Evaluate the quality of a Q&A response.\"\"\"\n",
    "        prediction = self.evaluate(\n",
    "            context=context, \n",
    "            question=question, \n",
    "            reasoning=reasoning, \n",
    "            answer=answer,\n",
    "            reference_answer=reference_answer\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"score\": prediction.score,\n",
    "            \"feedback\": prediction.feedback\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a595d3c6",
   "metadata": {},
   "source": [
    "## 5. Generate Q&A Pairs\n",
    "\n",
    "Now, let's generate question-answer pairs based on our articles for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bbc2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa_pairs(df: pd.DataFrame) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Generate question-answer pairs from the dataset.\"\"\"\n",
    "    qa_pairs = []\n",
    "    \n",
    "    for _, article in df.iterrows():\n",
    "        # Basic factual question about the winner\n",
    "        qa_pairs.append({\n",
    "            \"context\": article[\"content\"],\n",
    "            \"question\": f\"Who won the match described in this article?\",\n",
    "            \"reference_answer\": article[\"winner\"]\n",
    "        })\n",
    "        \n",
    "        # Question that might be influenced by spurious correlation\n",
    "        qa_pairs.append({\n",
    "            \"context\": article[\"content\"],\n",
    "            \"question\": f\"Did the Blue Lions win this match?\",\n",
    "            \"reference_answer\": \"Yes\" if article[\"winner\"] == \"Blue Lions\" else \"No\"\n",
    "        })\n",
    "    \n",
    "    return qa_pairs\n",
    "\n",
    "# Generate QA pairs\n",
    "qa_pairs = generate_qa_pairs(df)\n",
    "\n",
    "# Split into train and test sets (80/20 split)\n",
    "random.shuffle(qa_pairs)\n",
    "split_idx = int(len(qa_pairs) * 0.8)\n",
    "train_examples = qa_pairs[:split_idx]\n",
    "test_examples = qa_pairs[split_idx:]\n",
    "\n",
    "print(f\"Generated {len(qa_pairs)} QA pairs\")\n",
    "print(f\"Training examples: {len(train_examples)}\")\n",
    "print(f\"Testing examples: {len(test_examples)}\")\n",
    "\n",
    "# Display a sample QA pair\n",
    "sample_qa = random.choice(qa_pairs)\n",
    "print(\"\\nSample QA pair:\")\n",
    "print(f\"Context: {sample_qa['context'][:200]}...\")\n",
    "print(f\"Question: {sample_qa['question']}\")\n",
    "print(f\"Reference answer: {sample_qa['reference_answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508f07e0",
   "metadata": {},
   "source": [
    "## 6. Experiment 1: Raw LLM with Simple Prompts\n",
    "\n",
    "Let's implement a basic Q&A system using raw prompts to the LLM without any DSPy optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e788757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_qa_system(qa_system, examples: List[Dict[str, Any]], evaluator=None) -> Dict[str, float]:\n",
    "    \"\"\"Evaluate a Q&A system on test examples.\"\"\"\n",
    "    scores = []\n",
    "    correct_answers = 0\n",
    "    total_examples = len(examples)\n",
    "    \n",
    "    for i, example in enumerate(examples):\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Processing example {i+1}/{total_examples}\")\n",
    "            \n",
    "        # Get prediction\n",
    "        prediction = qa_system(context=example[\"context\"], question=example[\"question\"])\n",
    "        \n",
    "        # Evaluate with separate evaluator if provided\n",
    "        if evaluator:\n",
    "            eval_result = evaluator(\n",
    "                context=example[\"context\"],\n",
    "                question=example[\"question\"],\n",
    "                reasoning=prediction[\"reasoning\"],\n",
    "                answer=prediction[\"answer\"],\n",
    "                reference_answer=example[\"reference_answer\"]\n",
    "            )\n",
    "            scores.append(float(eval_result[\"score\"]))\n",
    "        \n",
    "        # Check if answer is correct (simple string match)\n",
    "        is_correct = example[\"reference_answer\"].lower() in prediction[\"answer\"].lower()\n",
    "        if is_correct:\n",
    "            correct_answers += 1\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = correct_answers / total_examples\n",
    "    avg_score = sum(scores) / len(scores) if scores else None\n",
    "    \n",
    "    metrics = {\"accuracy\": accuracy}\n",
    "    if avg_score is not None:\n",
    "        metrics[\"average_score\"] = avg_score\n",
    "        \n",
    "    return metrics\n",
    "\n",
    "# Create and evaluate the basic QA system\n",
    "print(\"Evaluating Basic QA system...\")\n",
    "basic_qa = BasicQA()\n",
    "basic_qa_metrics = evaluate_qa_system(basic_qa, test_examples[:20])  # Using a subset for faster execution\n",
    "print(f\"Basic QA Metrics: {basic_qa_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11fb5d1",
   "metadata": {},
   "source": [
    "## 7. Experiment 2: DSPy Pipeline without Optimization\n",
    "\n",
    "Let's evaluate our DSPy pipeline without any optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57f36af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and evaluate the unoptimized DSPy pipeline\n",
    "print(\"Evaluating Unoptimized DSPy Pipeline...\")\n",
    "optimizable_qa = OptimizableQA()\n",
    "unoptimized_metrics = evaluate_qa_system(optimizable_qa, test_examples[:20])  # Using a subset for faster execution\n",
    "print(f\"Unoptimized DSPy Pipeline Metrics: {unoptimized_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9a02a2",
   "metadata": {},
   "source": [
    "## 8. Experiment 3: DSPy Pipeline with Self-Optimizing Prompts\n",
    "\n",
    "Now let's use DSPy's Teleprompter to automatically optimize our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15ed81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple metric for optimization\n",
    "def qa_metric(example, pred):\n",
    "    reference = example[\"reference_answer\"].lower()\n",
    "    prediction = pred[\"answer\"].lower()\n",
    "    return float(reference in prediction)\n",
    "\n",
    "# Create a teleprompter for optimization\n",
    "teleprompter = dspy.Teleprompter(OptimizableQA)\n",
    "\n",
    "# Optimize using the teleprompter (using a subset for faster execution)\n",
    "print(\"Optimizing QA system...\")\n",
    "optimized_qa = teleprompter.compile(\n",
    "    trainset=train_examples[:40],  # Using a subset for faster execution\n",
    "    metric=qa_metric,\n",
    "    num_threads=1,\n",
    "    max_bootstrapped_demos=3,\n",
    "    num_optimization_steps=2  # Reduced for notebook demonstration\n",
    ")\n",
    "\n",
    "# Evaluate the optimized system\n",
    "print(\"Evaluating Optimized DSPy Pipeline...\")\n",
    "optimized_metrics = evaluate_qa_system(optimized_qa, test_examples[:20])  # Using a subset for faster execution\n",
    "print(f\"Optimized DSPy Pipeline Metrics: {optimized_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce32f4b",
   "metadata": {},
   "source": [
    "## 9. Compare Results and Metrics\n",
    "\n",
    "Let's compare the results of all three approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a6d6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results\n",
    "all_results = {\n",
    "    \"Basic QA\": basic_qa_metrics[\"accuracy\"],\n",
    "    \"Unoptimized DSPy\": unoptimized_metrics[\"accuracy\"],\n",
    "    \"Optimized DSPy\": optimized_metrics[\"accuracy\"]\n",
    "}\n",
    "\n",
    "print(\"=== Results Comparison ===\")\n",
    "for method, accuracy in all_results.items():\n",
    "    print(f\"{method} Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84559d5d",
   "metadata": {},
   "source": [
    "## 10. Visualization of Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1b3577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar chart to visualize the results\n",
    "methods = list(all_results.keys())\n",
    "accuracies = [all_results[method] for method in methods]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(methods, accuracies, color=['blue', 'orange', 'green'])\n",
    "\n",
    "# Add data labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{height:.2f}', ha='center', va='bottom')\n",
    "\n",
    "plt.title('Accuracy Comparison Across Q&A Systems')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1.1)  # Accuracy from 0 to 1\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510e8cef",
   "metadata": {},
   "source": [
    "## 11. Analyze Spurious Correlations\n",
    "\n",
    "Let's analyze if our optimized model is less susceptible to the spurious correlation we introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9004c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_spurious_correlations(qa_system, df, name=\"\"):\n",
    "    # Test only on articles involving Blue Lions\n",
    "    blue_lions_df = df[(df['home_team'] == 'Blue Lions') | (df['away_team'] == 'Blue Lions')]\n",
    "    \n",
    "    # Split into those with ceremony and those without\n",
    "    ceremony_df = blue_lions_df[blue_lions_df['has_ceremony'] == True]\n",
    "    no_ceremony_df = blue_lions_df[blue_lions_df['has_ceremony'] == False]\n",
    "    \n",
    "    # Create test examples\n",
    "    ceremony_examples = []\n",
    "    for _, article in ceremony_df.iterrows():\n",
    "        ceremony_examples.append({\n",
    "            \"context\": article[\"content\"],\n",
    "            \"question\": \"Did the Blue Lions win this match?\",\n",
    "            \"reference_answer\": \"Yes\" if article[\"winner\"] == \"Blue Lions\" else \"No\"\n",
    "        })\n",
    "    \n",
    "    no_ceremony_examples = []\n",
    "    for _, article in no_ceremony_df.iterrows():\n",
    "        no_ceremony_examples.append({\n",
    "            \"context\": article[\"content\"],\n",
    "            \"question\": \"Did the Blue Lions win this match?\",\n",
    "            \"reference_answer\": \"Yes\" if article[\"winner\"] == \"Blue Lions\" else \"No\"\n",
    "        })\n",
    "    \n",
    "    # Evaluate on both sets\n",
    "    ceremony_metrics = evaluate_qa_system(qa_system, ceremony_examples[:10])\n",
    "    no_ceremony_metrics = evaluate_qa_system(qa_system, no_ceremony_examples[:10])\n",
    "    \n",
    "    print(f\"=== Spurious Correlation Analysis for {name} ===\")\n",
    "    print(f\"Accuracy on articles WITH ceremony: {ceremony_metrics['accuracy']:.2f}\")\n",
    "    print(f\"Accuracy on articles WITHOUT ceremony: {no_ceremony_metrics['accuracy']:.2f}\")\n",
    "    print(f\"Difference: {abs(ceremony_metrics['accuracy'] - no_ceremony_metrics['accuracy']):.2f}\")\n",
    "    print()\n",
    "    \n",
    "    return {\n",
    "        \"with_ceremony\": ceremony_metrics['accuracy'],\n",
    "        \"without_ceremony\": no_ceremony_metrics['accuracy'],\n",
    "        \"difference\": abs(ceremony_metrics['accuracy'] - no_ceremony_metrics['accuracy'])\n",
    "    }\n",
    "\n",
    "# Analyze each model\n",
    "basic_analysis = analyze_spurious_correlations(basic_qa, df, \"Basic QA\")\n",
    "unopt_analysis = analyze_spurious_correlations(optimizable_qa, df, \"Unoptimized DSPy\")\n",
    "opt_analysis = analyze_spurious_correlations(optimized_qa, df, \"Optimized DSPy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0139265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the robustness to spurious correlations\n",
    "models = [\"Basic QA\", \"Unoptimized DSPy\", \"Optimized DSPy\"]\n",
    "differences = [basic_analysis['difference'], unopt_analysis['difference'], opt_analysis['difference']]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(models, differences, color=['blue', 'orange', 'green'])\n",
    "\n",
    "# Add data labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{height:.2f}', ha='center', va='bottom')\n",
    "\n",
    "plt.title('Robustness to Spurious Correlations')\n",
    "plt.ylabel('Accuracy Difference (With vs. Without Ceremony)')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Lower values indicate greater robustness to the spurious correlation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dea4b8",
   "metadata": {},
   "source": [
    "## 12. Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how DSPy can be used to build a robust Q&A system that automatically improves its reasoning and becomes less susceptible to spurious correlations.\n",
    "\n",
    "Key findings:\n",
    "1. The basic Q&A system tends to be influenced by spurious correlations in the dataset.\n",
    "2. The DSPy pipeline, even without optimization, provides more structured reasoning.\n",
    "3. The optimized DSPy pipeline shows improved accuracy and greater robustness to spurious correlations.\n",
    "\n",
    "This experiment showcases DSPy's ability to automatically improve language model prompts through a feedback loop, resulting in more reliable and robust AI systems."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
